# A4 ä½œæ¥­æ¶æ§‹ç¸½è¦½èˆ‡å¯¦ä½œæŒ‡å—

## ğŸ“‹ ç›®éŒ„
1. [æ•´é«”æ¶æ§‹](#architecture)
2. [å„çµ„ä»¶è©³è§£](#components)
3. [å¯¦ä½œæ­¥é©Ÿ](#steps)
4. [å¸¸è¦‹å•é¡Œ](#faq)
5. [è©•åˆ†æ¨™æº–](#grading)

---

## ğŸ—ï¸ æ•´é«”æ¶æ§‹ {#architecture}

```
Image Captioning Model æ¶æ§‹

                    Input Image
                         |
                         v
              +-------------------+
              | Vision Transformer |  â† é è¨“ç·´ ViT (å‡çµ/éƒ¨åˆ†è§£å‡)
              | (Image Encoder)    |
              +-------------------+
                         |
                    Visual Features
                    [B, 197, 1024]
                         |
                         v
              +-------------------+
              |  Linear Projection |  â† æŠ•å½±åˆ° d_model
              +-------------------+
                         |
                    Encoder Output
                    [B, 197, 512]
                         |
                         v
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    |                                           |
    |         Transformer Decoder               |
    |  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    |
    |  â”‚  Token Embedding                 â”‚    |
    |  â”‚  + Positional Encoding           â”‚    |
    |  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    |
    |                 |                         |
    |  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     |
    |  â”‚ Decoder Layer 1                 â”‚     |
    |  â”‚  â€¢ Masked Self-Attention        â”‚     |
    |  â”‚  â€¢ Cross-Attention              â”‚     |
    |  â”‚  â€¢ Feed-Forward Network         â”‚     |
    |  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     |
    |                |                         |
    |  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     |
    |  â”‚ Decoder Layer 2-N               â”‚     |
    |  â”‚  â€¢ ...                          â”‚     |
    |  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     |
    |                |                         |
    |  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     |
    |  â”‚ Layer Normalization             â”‚     |
    |  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    |
    |                                           |
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        |
                        v
              +-------------------+
              | Language Model    |  â† Linear layer to vocab
              | Head              |
              +-------------------+
                        |
                        v
                   Output Logits
                   [B, T, vocab_size]
                        |
                        v
                  Generated Caption
```

---

## ğŸ”§ å„çµ„ä»¶è©³è§£ {#components}

### 1ï¸âƒ£ Positional Encoding

**ä½œç”¨**: ç‚º token åºåˆ—æ³¨å…¥ä½ç½®è³‡è¨Š

**å…¬å¼**:
```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**é—œéµé»**:
- å¶æ•¸ç¶­åº¦ä½¿ç”¨ sine,å¥‡æ•¸ç¶­åº¦ä½¿ç”¨ cosine
- ä½¿ç”¨ `register_buffer` è¨»å†Šç‚ºéè¨“ç·´åƒæ•¸
- åœ¨ forward æ™‚åŠ åˆ° input embeddings ä¸Š

---

### 2ï¸âƒ£ Multi-Head Attention

**ä½œç”¨**: è®“æ¨¡å‹é—œæ³¨åºåˆ—ä¸­ä¸åŒä½ç½®çš„è³‡è¨Š

**æµç¨‹**:
```
Input: Q, K, V [batch, seq_len, d_model]
  |
  v
Linear Projections (Q_proj, K_proj, V_proj)
  |
  v
Split into H heads: [batch, num_heads, seq_len, head_dim]
  |
  v
Scaled Dot-Product Attention:
  scores = (Q @ K^T) / sqrt(head_dim)
  attn = softmax(masked_fill(scores))
  output = attn @ V
  |
  v
Concatenate heads: [batch, seq_len, d_model]
  |
  v
Output Projection
```

**é—œéµåƒæ•¸**:
- `d_model`: æ¨¡å‹ç¶­åº¦ (512)
- `num_heads`: æ³¨æ„åŠ›é ­æ•¸ (8)
- `head_dim = d_model // num_heads` (64)

---

### 3ï¸âƒ£ Feed-Forward Network

**çµæ§‹**:
```
Input [B, T, d_model]
  |
  v
Linear(d_model â†’ dim_ff)  # æ“´å±•
  |
  v
ReLU()
  |
  v
Dropout()
  |
  v
Linear(dim_ff â†’ d_model)  # å£“ç¸®
  |
  v
Dropout()
  |
  v
Output [B, T, d_model]
```

**ä½œç”¨**: å°æ¯å€‹ä½ç½®ç¨ç«‹é€²è¡Œéç·šæ€§è®Šæ›

---

### 4ï¸âƒ£ Transformer Decoder Layer

**çµ„æˆ**:
```
Input x [B, T, d_model]
  |
  â”œâ”€> LayerNorm â”€> Masked Self-Attention â”€â”€â”
  |                                         |
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  |
  â”œâ”€> LayerNorm â”€> Cross-Attention â”€â”€â”€â”€â”€â”€â”€â”€â”
  |                (attend to encoder)     |
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  |
  â”œâ”€> LayerNorm â”€> Feed-Forward â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  |                                         |
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  |
  v
Output x [B, T, d_model]
```

**ä¸‰å€‹ä¸»è¦æ“ä½œ**:
1. **Masked Self-Attention**: é˜²æ­¢çœ‹åˆ°æœªä¾†çš„ tokens
2. **Cross-Attention**: å°‡åœ–åƒç‰¹å¾µèå…¥åˆ°æ–‡å­—ç”Ÿæˆä¸­
3. **Feed-Forward**: ä½ç½®ç¨ç«‹çš„éç·šæ€§è®Šæ›

---

### 5ï¸âƒ£ Greedy Decoding

**ç”Ÿæˆæµç¨‹**:
```
1. åˆå§‹åŒ–: generated = [BOS]
2. Loop (max_len æ¬¡):
   a. å°‡ generated tokens è½‰ç‚º embeddings
   b. åŠ ä¸Š positional encoding
   c. é€šé decoder (ä½¿ç”¨ causal mask)
   d. å–æœ€å¾Œä¸€å€‹ä½ç½®çš„ logits
   e. é¸æ“‡æ©Ÿç‡æœ€é«˜çš„ token: next_token = argmax(logits[-1])
   f. å°‡ next_token åŠ å…¥ generated
   g. å¦‚æœ next_token == EOS,åœæ­¢
3. è¿”å›: generated caption
```

**Causal Mask**:
```
åœ¨æ™‚é–“æ­¥ t,åªèƒ½çœ‹åˆ° t åŠä¹‹å‰çš„ tokens
Mask matrix (ä¸Šä¸‰è§’ç‚º True):
    t0  t1  t2  t3
t0  0   1   1   1
t1  0   0   1   1
t2  0   0   0   1
t3  0   0   0   0
```

---

## ğŸ“ å¯¦ä½œæ­¥é©Ÿ {#steps}

### Step 1: æº–å‚™ç’°å¢ƒ âœ…
- å®‰è£å¿…è¦å¥—ä»¶
- è¼‰å…¥ dataset
- è¨­å®š device

### Step 2: è¨­å®š Special Tokens âœ…
```python
tokenizer.add_special_tokens({
    'bos_token': '<BOS>',
    'eos_token': '<EOS>',
    'pad_token': '<PAD>'
})
```

### Step 3: å¯¦ä½œ Positional Encoding âœ…
- è¨ˆç®— sinusoidal encoding
- è¨»å†Šç‚º buffer
- åœ¨ forward ä¸­åŠ åˆ° input

### Step 4: å¯¦ä½œ Multi-Head Attention âœ…
- Q, K, V æŠ•å½±
- åˆ†å‰²æˆå¤šå€‹ heads
- Scaled dot-product attention
- ä¸²æ¥ä¸¦æŠ•å½±å› d_model

### Step 5: å¯¦ä½œ Feed-Forward Network âœ…
- å…©å±¤ç·šæ€§å±¤
- ä¸­é–“åŠ  ReLU å’Œ Dropout

### Step 6: å¯¦ä½œ Decoder Layer âœ…
- Masked self-attention
- Cross-attention
- Feed-forward
- æ®˜å·®é€£æ¥ + LayerNorm

### Step 7: çµ„è£å®Œæ•´ Decoder âœ…
- å †ç–Šå¤šå±¤ decoder layers
- æœ€å¾ŒåŠ  LayerNorm

### Step 8: å¯¦ä½œç”Ÿæˆå‡½æ•¸ âœ…
- Greedy decoding
- è‡ªå›æ­¸ç”Ÿæˆ
- ä½¿ç”¨ causal mask

### Step 9: å®šç¾©è¨“ç·´è¨­å®š âœ…
- Loss: CrossEntropyLoss
- Optimizer: AdamW
- Learning rate

### Step 10: å¯¦ä½œè¨“ç·´å’Œè©•ä¼° âœ…
- Training loop
- Gradient clipping
- BLEU evaluation

---

## â“ å¸¸è¦‹å•é¡Œ {#faq}

### Q1: CUDA out of memory æ€éº¼è¾¦?
**A**: 
1. æ¸›å° `BATCH_SIZE` (ä¾‹å¦‚å¾ 32 â†’ 16)
2. æ¸›å°æ¨¡å‹å¤§å° (`D_MODEL`, `N_LAYERS`)
3. ä½¿ç”¨ gradient accumulation
4. åœ¨ Colab ä¸­: Runtime â†’ Factory reset runtime

### Q2: è¨“ç·´å¾ˆæ…¢æ€éº¼è¾¦?
**A**:
1. ç¢ºä¿ä½¿ç”¨ GPU (æª¢æŸ¥ `device`)
2. æ¸›å° `MAX_LEN`
3. å‡çµ ViT (ä¸è¦ fine-tune)
4. ä½¿ç”¨æ›´å°çš„ ViT æ¨¡å‹

### Q3: BLEU åˆ†æ•¸å¾ˆä½æ€éº¼è¾¦?
**A**:
1. æª¢æŸ¥ special tokens æ˜¯å¦æ­£ç¢ºè¨­å®š
2. å¢åŠ è¨“ç·´ epochs
3. èª¿æ•´å­¸ç¿’ç‡
4. æª¢æŸ¥ generate_greedy å¯¦ä½œ
5. ç¢ºä¿ eval æ™‚æ­£ç¢ºä½¿ç”¨ BOS/EOS

### Q4: Loss ä¸ä¸‹é™æˆ– NaN?
**A**:
1. é™ä½å­¸ç¿’ç‡
2. æª¢æŸ¥ gradient clipping
3. æª¢æŸ¥ attention mask æ˜¯å¦æ­£ç¢º
4. ç¢ºä¿ PAD token åœ¨ loss ä¸­è¢« ignore

### Q5: å¦‚ä½•æå‡åˆ†æ•¸?
**A**:
1. è§£å‡ ViT æœ€å¾Œå¹¾å±¤ fine-tune
2. å¢åŠ è¨“ç·´ epochs (10-15)
3. ä½¿ç”¨ learning rate scheduler
4. å¢åŠ æ¨¡å‹å®¹é‡ (D_MODEL, N_LAYERS)
5. å¯¦ä½œ beam search ä»£æ›¿ greedy
6. è³‡æ–™å¢å¼·

---

## ğŸ“Š è©•åˆ†æ¨™æº– {#grading}

### TODO å¯¦ä½œ: 80 åˆ†
- TODO 1: Special tokens (å¿…é ˆ)
- TODO 2: Positional Encoding (5 åˆ†)
- TODO 3: Multi-Head Attention (10 åˆ†)
- TODO 4: Feed-Forward (5 åˆ†)
- TODO 5: Decoder Layer (8 åˆ†)
- TODO 6: Decoder (5 åˆ†)
- TODO 7: Generate function (10 åˆ†)
- TODO 8: Loss & Optimizer (å¿…é ˆ)
- TODO 9: Training loop (15 åˆ†)
- TODO 10: Evaluation (15 åˆ†)
- å…¶ä»–: Setup, Model assembly (7 åˆ†)

### å ±å‘Š: 10 åˆ†
éœ€åŒ…å«:
- å¯¦ä½œèªªæ˜
- è¨­è¨ˆé¸æ“‡ (ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ)
- é‡åˆ°çš„å•é¡Œèˆ‡è§£æ±ºæ–¹å¼
- è¨“ç·´çµæœåˆ†æ

### åŠ åˆ†é¡Œ: 10 åˆ†
æ ¹æ“š test set ä¸Šçš„ sacreBLEU åˆ†æ•¸æ’å:
- Top 10%: +10 åˆ†
- 10-30%: +8 åˆ†
- 30-50%: +6 åˆ†
- 50-70%: +4 åˆ†
- 70-90%: +2 åˆ†
- Bottom 10%: +0 åˆ†

---

## ğŸ¯ å¯¦ä½œæª¢æŸ¥æ¸…å–®

### å¿…åšé …ç›®
- [ ] å¡«å¯«å§“åå’Œå­¸è™Ÿ
- [ ] TODO 1-10 å…¨éƒ¨å®Œæˆ
- [ ] ç¨‹å¼èƒ½æˆåŠŸåŸ·è¡Œ
- [ ] å®Œæˆè¨“ç·´ä¸¦å¾—åˆ°çµæœ
- [ ] æ’°å¯«å ±å‘Š

### æå‡åˆ†æ•¸ (é¸åš)
- [ ] Fine-tune ViT éƒ¨åˆ†å±¤
- [ ] ä½¿ç”¨ learning rate scheduler
- [ ] èª¿æ•´è¶…åƒæ•¸ç²å¾—æ›´å¥½çµæœ
- [ ] å¯¦ä½œ beam search
- [ ] åŠ å…¥è³‡æ–™å¢å¼·

---

## ğŸ’¡ å¯¦ç”¨æŠ€å·§

### 1. å¿«é€Ÿæ¸¬è©¦
```python
# ç”¨å° batch å…ˆæ¸¬è©¦æ•´å€‹ pipeline
test_batch = next(iter(train_loader))
logits, x_tgt = model(
    test_batch["pixel_values"][:2].to(device),
    test_batch["input_ids"][:2].to(device)
)
print(f"Logits shape: {logits.shape}")
print(f"Target shape: {x_tgt.shape}")
```

### 2. ç›£æ§è¨“ç·´
```python
# åœ¨ training loop ä¸­åŠ å…¥
if (i + 1) % 10 == 0:
    print(f"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}")
```

### 3. å„²å­˜æ¨¡å‹
```python
# åœ¨æ¯å€‹ epoch çµæŸå¾Œ
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': train_loss,
}, f'checkpoint_epoch_{epoch}.pt')
```

### 4. è¦–è¦ºåŒ–ç”Ÿæˆçµæœ
```python
# éš¨æ©Ÿé¸å¹¾å¼µåœ–çœ‹çœ‹ç”Ÿæˆçš„ caption
import random
idx = random.randint(0, len(test_loader.dataset))
sample = test_loader.dataset[idx]
image = sample['image']
generated = model.generate_greedy(
    image_processor(image, return_tensors='pt')['pixel_values'].to(device),
    tokenizer.bos_token_id,
    tokenizer.eos_token_id
)
caption = tokenizer.decode(generated[0], skip_special_tokens=True)
print(f"Generated: {caption}")
```

---

## ğŸ“š åƒè€ƒè³‡æº

1. **Transformer åŸè«–æ–‡**: "Attention Is All You Need"
2. **ViT è«–æ–‡**: "An Image is Worth 16x16 Words"
3. **Hugging Face Transformers æ–‡æª”**
4. **PyTorch å®˜æ–¹æ•™å­¸**

---

## ğŸ‰ ç¥ä½ ä½œæ¥­é †åˆ©!

è¨˜å¾—:
1. å…ˆå®Œæˆæ‰€æœ‰ TODO
2. ç¢ºä¿ç¨‹å¼èƒ½è·‘
3. èª¿æ•´åƒæ•¸æå‡åˆ†æ•¸
4. æ’°å¯«å ±å‘Šèªªæ˜ä½ çš„å¯¦ä½œ

æœ‰å•é¡Œå¯ä»¥åƒè€ƒ:
- `A4_TODO_Solutions.md` - è©³ç´°çš„è§£æ±ºæ–¹æ¡ˆèªªæ˜
- `A4_Code_Snippets.py` - å¯ç›´æ¥è¤‡è£½çš„ä»£ç¢¼ç‰‡æ®µ

Good luck! ğŸš€
