# A4 ä½œæ¥­ TODO å®ŒæˆæŒ‡å—

é€™ä»½æ–‡ä»¶åŒ…å«äº† A4.ipynb ä¸­æ‰€æœ‰ TODO é …ç›®çš„å®Œæ•´è§£æ±ºæ–¹æ¡ˆã€‚

## ç›®éŒ„
1. [Special Tokens è¨­ç½®](#todo-1)
2. [Positional Encoding å¯¦ä½œ](#todo-2)
3. [Multi-Head Attention å¯¦ä½œ](#todo-3)
4. [FeedForward Network å¯¦ä½œ](#todo-4)
5. [Transformer Decoder Layer å¯¦ä½œ](#todo-5)
6. [Transformer Decoder å¯¦ä½œ](#todo-6)
7. [Greedy Decoding ç”Ÿæˆå‡½æ•¸](#todo-7)
8. [Loss å’Œ Optimizer å®šç¾©](#todo-8)
9. [Training Loop å¯¦ä½œ](#todo-9)
10. [Evaluation BLEU å¯¦ä½œ](#todo-10)

---

## TODO 1: Special Tokens è¨­ç½® {#todo-1}

**ä½ç½®**: Cell 17 (åœ¨ `print("After updating special tokens:")` ä¹‹å¾Œ)

**è¦åšä»€éº¼**: ç‚º tokenizer æ·»åŠ  BOS (beginning of sequence), EOS (end of sequence), å’Œ PAD (padding) ç‰¹æ®Š tokensã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# Add special tokens for BOS, EOS, and PAD
tokenizer.add_special_tokens({
    'bos_token': '<BOS>',
    'eos_token': '<EOS>',
    'pad_token': '<PAD>'
})
```

**èªªæ˜**:
- ä½¿ç”¨ `add_special_tokens` æ–¹æ³•å°‡ä¸‰å€‹ç‰¹æ®Š token åŠ å…¥ tokenizer
- `<BOS>` æ¨™è¨˜åºåˆ—é–‹å§‹
- `<EOS>` æ¨™è¨˜åºåˆ—çµæŸ
- `<PAD>` ç”¨æ–¼å¡«å……åºåˆ—è‡³ç›¸åŒé•·åº¦

---

## TODO 2: Positional Encoding å¯¦ä½œ {#todo-2}

**ä½ç½®**: Cell 24 (PositionalEncoding class)

**è¦åšä»€éº¼**: å¯¦ä½œ sinusoidal positional encoding,é€™æ˜¯ Transformer åŸè«–æ–‡æå‡ºçš„æ–¹æ³•ã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def __init__(self, d_model: int, max_len: int = 512):
    super().__init__()
    # Create positional encoding matrix
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    pe = pe.unsqueeze(0)  # [1, max_len, d_model]
    self.register_buffer('pe', pe)

def forward(self, x: torch.Tensor) -> torch.Tensor:
    """
    x: input tensor of shape [batch_size, seq_len, d_model]
    Adds positional encoding to input tensor, up to seq_len.
    """
    B, T, D = x.shape
    return x + self.pe[:, :T, :]
```

**èªªæ˜**:
- ä½¿ç”¨ sine å’Œ cosine å‡½æ•¸ç”Ÿæˆä½ç½®ç·¨ç¢¼
- å¶æ•¸ç¶­åº¦ä½¿ç”¨ sine,å¥‡æ•¸ç¶­åº¦ä½¿ç”¨ cosine
- ä½¿ç”¨ `register_buffer` è¨»å†Šç‚ºéå¯è¨“ç·´åƒæ•¸
- åœ¨ forward ä¸­å°‡ä½ç½®ç·¨ç¢¼åŠ åˆ°è¼¸å…¥ embedding ä¸Š

---

## TODO 3: Multi-Head Attention å¯¦ä½œ {#todo-3}

**ä½ç½®**: Cell 24 (MultiHeadAttention class)

**è¦åšä»€éº¼**: å¯¦ä½œ scaled dot-product multi-head attention æ©Ÿåˆ¶ã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):
    super().__init__()
    assert d_model % num_heads == 0
    self.d_model = d_model
    self.num_heads = num_heads
    self.head_dim = d_model // num_heads
    
    self.q_proj = nn.Linear(d_model, d_model)
    self.k_proj = nn.Linear(d_model, d_model)
    self.v_proj = nn.Linear(d_model, d_model)
    self.out_proj = nn.Linear(d_model, d_model)
    self.dropout = nn.Dropout(dropout)

def _shape(self, x: torch.Tensor, B: int, T: int) -> torch.Tensor:
    """
    x reshape:
    [batch, seq_len, d_model] -> [batch, num_heads, seq_len, head_dim]
    """
    return x.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)

def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, attn_mask: Optional[torch.Tensor] = None):
    """
    Multi-Head Attention forward pass.
    """
    B, Tq, _ = Q.shape
    _, Tk, _ = K.shape
    
    # Project Q, K, V
    Q = self.q_proj(Q)
    K = self.k_proj(K)
    V = self.v_proj(V)
    
    # Reshape to [B, num_heads, T, head_dim]
    Q = self._shape(Q, B, Tq)
    K = self._shape(K, B, Tk)
    V = self._shape(V, B, Tk)
    
    # Scaled dot-product attention
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
    
    if attn_mask is not None:
        scores = scores.masked_fill(attn_mask, float('-inf'))
    
    attn_weights = F.softmax(scores, dim=-1)
    attn_weights = self.dropout(attn_weights)
    
    # Apply attention to values
    attn_output = torch.matmul(attn_weights, V)
    
    # Reshape back: [B, num_heads, Tq, head_dim] -> [B, Tq, d_model]
    attn_output = attn_output.transpose(1, 2).contiguous().view(B, Tq, self.d_model)
    
    # Final projection
    out = self.out_proj(attn_output)
    return out
```

**èªªæ˜**:
- ä½¿ç”¨ç·šæ€§å±¤æŠ•å½± Q, K, V
- å°‡ d_model åˆ†å‰²æˆå¤šå€‹ heads (num_heads Ã— head_dim)
- è¨ˆç®— scaled dot-product attention: `scores = QK^T / sqrt(head_dim)`
- æ‡‰ç”¨ attention mask (ç”¨æ–¼ causal attention)
- å°‡å¤šå€‹ heads çš„è¼¸å‡ºä¸²æ¥ä¸¦æŠ•å½±å› d_model

---

## TODO 4: FeedForward Network å¯¦ä½œ {#todo-4}

**ä½ç½®**: Cell 24 (FeedForward class)

**è¦åšä»€éº¼**: å¯¦ä½œ position-wise feedforward networkã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def __init__(self, d_model: int, dim_ff: int, dropout: float = 0.0):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(d_model, dim_ff),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(dim_ff, d_model),
        nn.Dropout(dropout)
    )
```

**èªªæ˜**:
- å…©å±¤å…¨é€£æ¥ç¶²è·¯
- ç¬¬ä¸€å±¤å°‡ç¶­åº¦å¾ d_model æ“´å±•åˆ° dim_ff
- ä½¿ç”¨ ReLU å•Ÿå‹•å‡½æ•¸
- ç¬¬äºŒå±¤å°‡ç¶­åº¦æŠ•å½±å› d_model
- åŠ å…¥ dropout é˜²æ­¢éæ“¬åˆ

---

## TODO 5: Transformer Decoder Layer å¯¦ä½œ {#todo-5}

**ä½ç½®**: Cell 24 (TransformerDecoderLayer class)

**è¦åšä»€éº¼**: å¯¦ä½œå–®å±¤ Transformer Decoder,åŒ…å« masked self-attention, cross-attention å’Œ FFNã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def __init__(self, d_model: int, num_heads: int, dim_ff: int, dropout: float = 0.1):
    super().__init__()
    self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
    self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
    self.ffn = FeedForward(d_model, dim_ff, dropout)
    
    self.norm1 = nn.LayerNorm(d_model)
    self.norm2 = nn.LayerNorm(d_model)
    self.norm3 = nn.LayerNorm(d_model)
    
    self.dropout = nn.Dropout(dropout)

def forward(self, x, enc, causal_mask: Optional[torch.Tensor] = None):
    """
    Args:
        x: Decoder input embeddings [batch, tgt_seq_len, d_model]
        enc: Encoder output representations [batch, src_seq_len, d_model]
        causal_mask: Optional attention mask for target-side self-attention
    """
    # Masked self-attention
    x_norm = self.norm1(x)
    attn_out = self.self_attn(x_norm, x_norm, x_norm, causal_mask)
    x = x + self.dropout(attn_out)
    
    # Cross-attention
    x_norm = self.norm2(x)
    attn_out = self.cross_attn(x_norm, enc, enc)
    x = x + self.dropout(attn_out)
    
    # Feed-forward
    x_norm = self.norm3(x)
    ffn_out = self.ffn(x_norm)
    x = x + self.dropout(ffn_out)
    
    return x
```

**èªªæ˜**:
- åŒ…å«ä¸‰å€‹ä¸»è¦çµ„ä»¶:
  1. **Masked Self-Attention**: decoder å°è‡ªèº«çš„ attention (å¸¶ causal mask)
  2. **Cross-Attention**: decoder å° encoder è¼¸å‡ºçš„ attention
  3. **Feed-Forward Network**: position-wise FFN
- æ¯å€‹çµ„ä»¶éƒ½ä½¿ç”¨ residual connection å’Œ Layer Normalization
- ä½¿ç”¨ Pre-LN æ¶æ§‹ (å…ˆ normalize å†åš attention/FFN)

---

## TODO 6: Transformer Decoder å¯¦ä½œ {#todo-6}

**ä½ç½®**: Cell 24 (TransformerDecoder class)

**è¦åšä»€éº¼**: å †ç–Šå¤šå±¤ TransformerDecoderLayerã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def __init__(self, d_model: int, num_layers: int, num_heads: int, dim_ff: int, dropout: float = 0.1):
    super().__init__()
    self.layers = nn.ModuleList([
        TransformerDecoderLayer(d_model, num_heads, dim_ff, dropout)
        for _ in range(num_layers)
    ])
    self.ln = nn.LayerNorm(d_model)

def forward(self, x, enc, causal_mask: Optional[torch.Tensor] = None):
    for layer in self.layers:
        x = layer(x, enc, causal_mask)
    return self.ln(x)
```

**èªªæ˜**:
- ä½¿ç”¨ `nn.ModuleList` å‰µå»ºå¤šå±¤ decoder layers
- ä¾åºé€šéæ¯ä¸€å±¤
- æœ€å¾ŒåŠ ä¸Š LayerNorm ç©©å®šè¼¸å‡º

---

## TODO 7: Greedy Decoding ç”Ÿæˆå‡½æ•¸ {#todo-7}

**ä½ç½®**: Cell 26 (CaptionModel.generate_greedy method)

**è¦åšä»€éº¼**: å¯¦ä½œ greedy decoding ç®—æ³•ä¾†ç”Ÿæˆ image captionsã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def generate_greedy(self, pixel_values: torch.Tensor, bos_id: int, eos_id: int, max_len: int = 32):
    """
    Greedy decoding for image-to-text generation.
    """
    B = pixel_values.shape[0]
    enc = self.encode_image(pixel_values)
    
    # Start with BOS token
    generated = torch.full((B, 1), bos_id, dtype=torch.long, device=pixel_values.device)
    
    for _ in range(max_len - 1):
        # Embed and add positional encoding
        x = self.token_emb(generated)
        x = self.pos_enc(x)
        
        # Create causal mask
        T = x.shape[1]
        causal_mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=x.device), diagonal=1)[None, None, :, :]
        
        # Decode
        dec = self.decoder(x, enc, causal_mask)
        logits = self.lm_head(dec)
        
        # Get next token (greedy)
        next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)
        generated = torch.cat([generated, next_token], dim=1)
        
        # Check if all sequences have generated EOS
        if (next_token == eos_id).all():
            break
    
    return generated
```

**èªªæ˜**:
- å¾ BOS token é–‹å§‹
- æ¯æ­¥é¸æ“‡æ©Ÿç‡æœ€é«˜çš„ token (greedy)
- è‡ªå›æ­¸ç”Ÿæˆ:æ¯æ¬¡å°‡æ–° token åŠ å…¥åºåˆ—
- ç•¶æ‰€æœ‰åºåˆ—éƒ½ç”Ÿæˆ EOS æˆ–é”åˆ° max_len æ™‚åœæ­¢

---

## TODO 8: Loss å’Œ Optimizer å®šç¾© {#todo-8}

**ä½ç½®**: Cell 31

**è¦åšä»€éº¼**: å®šç¾©æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨ã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
```

**èªªæ˜**:
- ä½¿ç”¨ AdamW optimizer (Adam with weight decay)
- ä½¿ç”¨ CrossEntropyLoss
- `ignore_index=tokenizer.pad_token_id` ç¢ºä¿ padding tokens ä¸åƒèˆ‡ loss è¨ˆç®—

---

## TODO 9: Training Loop å¯¦ä½œ {#todo-9}

**ä½ç½®**: Cell 33 (train function)

**è¦åšä»€éº¼**: å¯¦ä½œå®Œæ•´çš„è¨“ç·´å¾ªç’°ã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
def train():
    model.train()
    total = 0.0
    
    for batch in train_loader:
        pixel_values = batch["pixel_values"].to(device)
        input_ids = batch["input_ids"].to(device)
        
        # Forward pass
        logits, x_tgt = model(pixel_values, input_ids)
        
        # Compute loss
        loss = criterion(logits.reshape(-1, logits.shape[-1]), x_tgt.reshape(-1))
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # Update weights
        optimizer.step()
        
        total += loss.item()
    
    return total / max(1, len(train_loader))
```

**èªªæ˜**:
- è¨­ç½®æ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼
- éæ­·æ‰€æœ‰ training batches
- Forward pass è¨ˆç®— logits
- è¨ˆç®— cross-entropy loss
- Backward pass è¨ˆç®—æ¢¯åº¦
- ä½¿ç”¨ gradient clipping é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- æ›´æ–°æ¨¡å‹åƒæ•¸
- è¿”å›å¹³å‡ loss

---

## TODO 10: Evaluation BLEU å¯¦ä½œ {#todo-10}

**ä½ç½®**: Cell 33 (eval_bleu function å…§éƒ¨)

**è¦åšä»€éº¼**: å¯¦ä½œè©•ä¼°å¾ªç’°,ä½¿ç”¨æ¨¡å‹ç”Ÿæˆ captions ä¸¦è¨ˆç®— BLEU åˆ†æ•¸ã€‚

**è§£æ±ºæ–¹æ¡ˆ**:
```python
bos = tokenizer.bos_token_id
eos = tokenizer.eos_token_id
for batch in loader:
    pix = batch["pixel_values"].to(device)
    
    # Generate captions
    gen_ids = model.generate_greedy(pix, bos, eos, max_len=GEN_MAX_LEN)
    
    # Decode predictions
    pred_texts = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)
    preds.extend(pred_texts)
    
    # Get reference captions
    ref_texts = tokenizer.batch_decode(batch["input_ids"], skip_special_tokens=True)
    refs.extend([[r] for r in ref_texts])
```

**èªªæ˜**:
- ç²å– BOS å’Œ EOS token IDs
- éæ­· validation/test loader
- ä½¿ç”¨ `generate_greedy` ç”Ÿæˆé æ¸¬çš„ captions
- ä½¿ç”¨ tokenizer å°‡ token IDs è§£ç¢¼ç‚ºæ–‡å­—
- æ”¶é›†é æ¸¬å’Œåƒè€ƒ captions
- æ³¨æ„ refs éœ€è¦æ˜¯ list of lists æ ¼å¼ (æ¯å€‹æ¨£æœ¬å¯èƒ½æœ‰å¤šå€‹åƒè€ƒ captions)

---

## é¡å¤–å»ºè­°

### æå‡åˆ†æ•¸çš„ç­–ç•¥:

1. **è§£å‡éƒ¨åˆ† ViT å±¤**:
   - åœ¨ Cell 21 ä¸­,å¯ä»¥å˜—è©¦è§£å‡ ViT çš„æœ€å¾Œå¹¾å±¤é€²è¡Œ fine-tuning
   ```python
   # Unfreeze last few layers
   for name, param in vit.named_parameters():
       if 'encoder.layer.11' in name or 'encoder.layer.10' in name:
           param.requires_grad = True
   ```

2. **èª¿æ•´è¶…åƒæ•¸**:
   - å¢åŠ  `EPOCHS` (ä¾‹å¦‚ 10-15)
   - èª¿æ•´å­¸ç¿’ç‡ `LR` (å˜—è©¦ 1e-4 åˆ° 5e-4)
   - å¢åŠ æ¨¡å‹å®¹é‡:`D_MODEL`, `N_LAYERS`, `FFN_DIM`
   - èª¿æ•´ `BATCH_SIZE` ä»¥é©æ‡‰ GPU è¨˜æ†¶é«”

3. **ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨**:
   ```python
   from torch.optim.lr_scheduler import CosineAnnealingLR
   scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)
   ```

4. **è³‡æ–™å¢å¼·**:
   - åœ¨ image preprocessing ä¸­åŠ å…¥è³‡æ–™å¢å¼·

5. **ä½¿ç”¨æ›´å¥½çš„ decoding ç­–ç•¥**:
   - å¯¦ä½œ beam search è€Œé greedy decoding
   - ä½¿ç”¨ nucleus sampling (top-p sampling)

### å¸¸è¦‹å•é¡Œ:

1. **CUDA out of memory**:
   - æ¸›å° `BATCH_SIZE`
   - æ¸›å°æ¨¡å‹å¤§å° (`D_MODEL`, `N_LAYERS`)
   - ä½¿ç”¨æ¢¯åº¦ç´¯ç©

2. **è¨“ç·´ä¸ç©©å®š**:
   - æª¢æŸ¥ gradient clipping è¨­å®š
   - é™ä½å­¸ç¿’ç‡
   - å¢åŠ  warmup steps

3. **BLEU åˆ†æ•¸éä½**:
   - æª¢æŸ¥ tokenizer è¨­å®š
   - ç¢ºä¿ special tokens æ­£ç¢ºè™•ç†
   - å¢åŠ è¨“ç·´ epochs

---

## å®Œæˆæª¢æŸ¥æ¸…å–®

- [ ] TODO 1: Special tokens è¨­ç½®å®Œæˆ
- [ ] TODO 2: PositionalEncoding å¯¦ä½œå®Œæˆ
- [ ] TODO 3: MultiHeadAttention å¯¦ä½œå®Œæˆ
- [ ] TODO 4: FeedForward å¯¦ä½œå®Œæˆ
- [ ] TODO 5: TransformerDecoderLayer å¯¦ä½œå®Œæˆ
- [ ] TODO 6: TransformerDecoder å¯¦ä½œå®Œæˆ
- [ ] TODO 7: generate_greedy å¯¦ä½œå®Œæˆ
- [ ] TODO 8: Loss å’Œ Optimizer å®šç¾©å®Œæˆ
- [ ] TODO 9: Training loop å¯¦ä½œå®Œæˆ
- [ ] TODO 10: Evaluation å¯¦ä½œå®Œæˆ
- [ ] å¡«å¯«å§“åå’Œå­¸è™Ÿ
- [ ] å®Œæˆå ±å‘Š (10åˆ†)
- [ ] è¨“ç·´æ¨¡å‹ä¸¦æäº¤æ¸¬è©¦çµæœ

ç¥ä½ ä½œæ¥­é †åˆ©! ğŸ‰
